# é€šä¹‰åƒé—®å¤§æ¨¡å‹APIå°è£…ç±»
import json
from transformers import AutoTokenizer

class Qwen:
    def __init__(self, message: dict):
        self.api_key = message.get("key")
        self.base_url = message.get("params").get("base_url")
        self.model = message.get("params").get("model")
        # ================ å¯é€‰å‚æ•°è®¾ç½® ================
        self.stream_options = True # æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼
        self.temperature = 0.8 # æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œè¶Šå¤§è¶Šéšæœº
        self.max_tokens = message.get("params").get("max_tokens", 8000)  # ä»é…ç½®è¯»å–ï¼Œé»˜è®¤8000
        self.top_k = 5 # ä»kä¸ªå€™é€‰ä¸­éšæœºé€‰æ‹©ä¸€ä¸ª
        self.auditing = "default" # å®¡æ ¸è®¾ç½®

        # ================ æ¨¡å‹åç§°åˆ°HuggingFace tokenizerè·¯å¾„çš„æ˜ å°„ ================
        # APIæ¨¡å‹åç§°åˆ°HuggingFace tokenizerè·¯å¾„çš„æ˜ å°„
        tokenizer_map = {
            "qwen-turbo": "Qwen/Qwen-7B-Chat",
            "qwen-plus": "Qwen/Qwen-14B-Chat",
            "qwen-max": "Qwen/Qwen-72B-Chat",
            "qwen-max-longcontext": "Qwen/Qwen-72B-Chat",
        }
        # å¦‚æœmodelåœ¨æ˜ å°„è¡¨ä¸­ï¼Œä½¿ç”¨æ˜ å°„çš„è·¯å¾„ï¼›å¦åˆ™å‡å®šmodelæœ¬èº«å°±æ˜¯HuggingFaceè·¯å¾„
        tokenizer_path = tokenizer_map.get(self.model, "Qwen/Qwen-7B-Chat")
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)



        
    def set_api_key(self, api_key: str):
        self.api_key = api_key

    def set_base_url(self, base_url: str):
        self.base_url = base_url

    def set_model(self, model: str):
        self.model = model

    #  ============ ç”Ÿæˆé“¾æ¥å‚æ•° ============
    def gen_params(self):
        return {
            "api_key": self.api_key,
            "base_url": self.base_url,
        }
    #  ============ ç”Ÿæˆè¯·æ±‚å‚æ•° ============
    def gen_request(self, messages: list):
        """
        ç”Ÿæˆè¯·æ±‚å‚æ•°ï¼Œä»¿ç…§æ˜Ÿç« gen_params æ–¹æ³•
        ä¼šè¯»å–å’Œä¿å­˜å¯¹è¯å†å²ï¼Œå¹¶ç”Ÿæˆè¯·æ±‚ä½“
        """
        # ç”Ÿæˆè¯·æ±‚å‚æ•°
        return {
            "model": self.model,
            "messages": messages,  # Qwen/ChatGLMç­‰APIå‚æ•°åé€šå¸¸ä¸ºmessages
        }
        
    #  ============ ç”Ÿæˆè¯·æ±‚å‚æ•°(æµå¼) ============
    def gen_params_stream(self, messages: list):
        """
        ç”Ÿæˆæµå¼è¯·æ±‚å‚æ•°
        """
        return {
            "model": self.model,
            "messages": messages,  # Qwen/ChatGLMç­‰APIå‚æ•°åé€šå¸¸ä¸ºmessages
            "stream": True,
            "stream_options": {"include_usage": True}
        }

    #  ============ åˆ¤æ–­æµå¼æ˜¯å¦ç»“æŸ ============
    def is_stream_end(self, stream_options: dict) -> bool:
        """
        åˆ¤æ–­æµå¼æ˜¯å¦ç»“æŸ
        usage å­—æ®µä¸º None è¡¨ç¤ºè¿˜åœ¨ä¼ è¾“æ•°æ®ï¼Œå¦åˆ™ä¸ºç»“æŸ
        """
        # usage å­—æ®µä¸º Noneï¼Œåˆ™è¯´æ˜è¿˜åœ¨ä¼ è¾“ï¼ŒéNoneè§†ä¸ºç»“æŸ
        usage = stream_options.get("usage", None)
        return usage is not None
        
    #  ============ æå–æµå¼ä¿¡æ¯æ•°æ® ============
    def extract_stream_info(self, stream_options: dict) -> str:
        """
        æå–æµå¼ä¿¡æ¯æ•°æ®, æå–delta.contentå’Œdelta.thinkingå­—æ®µ
        ä¼˜å…ˆè¿”å›thinkingï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›contentï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰
        """
        # 'choices' åº”ä¸ºä¸€ä¸ªlist
        choices = stream_options.get('choices', [])
        if not choices or not isinstance(choices, list):
            return None
        choice = choices[0]
        delta = choice.get('delta', {}) if isinstance(choice, dict) else {}
        
        # ä¼˜å…ˆæå–thinkingå­—æ®µï¼ˆæ·±åº¦æ€è€ƒæ¨¡å¼ï¼‰
        thinking = delta.get('thinking', None)
        if thinking:
            return f"ğŸ’­ {thinking}"  # ç”¨ç‰¹æ®Šæ ‡è®°åŒºåˆ†æ€è€ƒè¿‡ç¨‹
        
        # å¦‚æœæ²¡æœ‰thinkingï¼Œæå–contentï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰
        return delta.get('content', None)

    #  ============ è®¡ç®—tokençš„å›è°ƒå‡½æ•° ============
    # é˜¿é‡Œï¼ˆé€šä¹‰åƒé—®ï¼šQwen ç³»åˆ—ï¼‰
    # å·¥å…·ï¼štransformersåº“åŠ è½½ Qwen çš„ tokenizerï¼ˆå¼€æºæ¨¡å‹ï¼‰æˆ–å®˜æ–¹ API çš„usageå­—æ®µ
    # åŸç†ï¼šåŸºäº BPEï¼Œä¸­æ–‡åˆ†è¯ç²’åº¦è¾ƒç»†ï¼ˆå•å­—æˆ–è¯ï¼‰ã€‚
    def token_callback(self, content: str) -> int:
            """è®¡ç®—é€šä¹‰åƒé—®æ¨¡å‹çš„tokenæ•°"""
            return len(self.tokenizer.encode(content, add_special_tokens=False))
